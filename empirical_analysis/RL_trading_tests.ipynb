{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from MDQN import M_DQN_Agent\n",
    "import torch\n",
    "from collections import deque\n",
    "from Trading_Simulator import TradingEnvironment, pca_res_gen, fourier_signal_extractor\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingEnvironment(financial_dataset=price_df,\n",
    "                        residual_generator=pca_res_gen,\n",
    "                        signal_generator=fourier_signal_extractor,\n",
    "                        episode_length=100,\n",
    "                        lookback_window=252,\n",
    "                        signal_window=60,\n",
    "                        transaction_costs=0.0,\n",
    "                        short_cost=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "filename = 'price_df.csv'\n",
    "if os.path.exists(filename):\n",
    "    price_df = pd.read_csv(filename)\n",
    "    price_df = price_df.rename(columns={'Unnamed: 0':'date'})\n",
    "    price_df.set_index('date', inplace=True)\n",
    "    price_df.index = pd.to_datetime(price_df.index)\n",
    "\n",
    "returns = price_df.diff(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PARAMETERS\n",
    "#seed = 100\n",
    "seed = np.random.randint(0,100000)\n",
    "\n",
    "#writer = SummaryWriter(\"runs/\"+\"DQN_LL_new_1\")\n",
    "frames = 1000\n",
    "BUFFER_SIZE = 1000000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "TAU = 1e-2\n",
    "eps_frames=5000\n",
    "min_eps=0.025\n",
    "LR = 1e-3\n",
    "UPDATE_EVERY = 1\n",
    "n_step = 1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using \", device)\n",
    "eps_fixed = False\n",
    "SAVE_MODEL = False\n",
    "file_name = 'N1'\n",
    "\n",
    "# Set seeds\n",
    "#env.seed(seed)\n",
    "#nv.action_space.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# action_size     = env.action_space.n #### going to fix this\n",
    "action_size       = 3\n",
    "#state_size        = env.observation_space.shape\n",
    "state_size        = [16]\n",
    "\n",
    "agent = M_DQN_Agent(state_size=state_size,    \n",
    "                        action_size=action_size,\n",
    "                        layer_size=64,\n",
    "                        BATCH_SIZE=BATCH_SIZE, \n",
    "                        BUFFER_SIZE=BUFFER_SIZE, \n",
    "                        LR=LR, \n",
    "                        TAU=TAU, \n",
    "                        GAMMA=GAMMA, \n",
    "                        UPDATE_EVERY=UPDATE_EVERY, \n",
    "                        device=device, \n",
    "                        seed=seed)\n",
    "\n",
    "##########################\n",
    "action_to_portfolio = {0:-1, 1:0, 2: 1}\n",
    "scores = []                        # list containing scores from each episode\n",
    "scores_window = deque(maxlen=100)  # last 100 scores\n",
    "output_history = []\n",
    "frame = 0\n",
    "if eps_fixed:\n",
    "    eps = 0\n",
    "else:\n",
    "    eps = 1\n",
    "eps_start = 1\n",
    "i_episode = 1\n",
    "\n",
    "env = TradingEnvironment(financial_dataset=price_df.dropna(axis=0, thresh=300),\n",
    "                        residual_generator=pca_res_gen,\n",
    "                        signal_generator=fourier_signal_extractor,\n",
    "                        episode_length=100,\n",
    "                        lookback_window=252,\n",
    "                        signal_window=60,\n",
    "                        transaction_costs=0.0,\n",
    "                        short_cost=0.0)\n",
    "\n",
    "state, _ = env.warm_up()\n",
    "score = 0\n",
    "\n",
    "for frame in range(1, frames+1):\n",
    "    action = agent.act_para(state, eps)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step([action_to_portfolio[action]])\n",
    "    agent.step(state, action, reward, next_state, done)#, writer)\n",
    "    state = next_state\n",
    "    score += reward\n",
    "    # linear annealing to the min epsilon value until eps_frames and from there slowly decease epsilon to 0 until the end of training\n",
    "    if eps_fixed == False:\n",
    "        if frame < eps_frames:\n",
    "            eps = max(eps_start - (frame*(1/eps_frames)), min_eps)\n",
    "        else:\n",
    "            eps = max(min_eps - min_eps*((frame-eps_frames)/(frames-eps_frames)), 0.001)\n",
    "    \n",
    "    if done:\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        #writer.add_scalar(\"Average100\", np.mean(scores_window), frame)\n",
    "        output_history.append(np.mean(scores_window))\n",
    "        env.evaluate_performance()\n",
    "        print('\\rEpisode {}\\tFrame {} \\tAverage Score: {:.2f}'.format(i_episode, frame, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tFrame {}\\tAverage Score: {:.2f}'.format(i_episode,frame, np.mean(scores_window)))\n",
    "        i_episode +=1 \n",
    "        score = 0  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
